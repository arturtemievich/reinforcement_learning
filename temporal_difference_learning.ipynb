{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference (TD) Learning\n",
    "\n",
    "If one had to identify one idea as central and novel to reinforcement learning, it would undoubtedly be **temporal-difference (TD) learning**\n",
    "\n",
    "TD learning is a combination of Monte Carlo ideas and Dynamic Programming ideas.\n",
    "\n",
    "> Like Monte Carlo methods, TD methods can learn directly from raw experience without a model of the environment’s dynamics.\n",
    "\n",
    "> Like DP, TD methods update estimates based in part on other learned estimates, without waiting for a ﬁnal outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import defaultdict\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning\n",
    "\n",
    "The state-action value update is given by:\n",
    "\n",
    "$$\n",
    "Q({S_t},{A_t}) \\leftarrow Q({S_t},{A_t}) + \\alpha \\left[{R_{t+1}} + \\gamma \\max\\limits_a Q({S_{t+1}},{a}) - Q({S_t},{A_t})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, alpha, epsilon, discount, get_legal_actions):\n",
    "        \"\"\"\n",
    "        Q-Learning Agent\n",
    "        Instance variables you have access to\n",
    "          - self.epsilon (exploration prob)\n",
    "          - self.alpha (learning rate)\n",
    "          - self.discount (discount rate aka gamma)\n",
    "\n",
    "        Functions you should use\n",
    "          - self.get_legal_actions(state) {state, hashable -> list of actions, each is hashable}\n",
    "            which returns legal actions for a state\n",
    "          - self.get_qvalue(state, action), which returns Q(state,action)\n",
    "          - self.set_qvalue(state, action, value), which sets Q(state,action) := value\n",
    "        \"\"\"\n",
    "\n",
    "        self.get_legal_actions = get_legal_actions\n",
    "        self._qvalues = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.discount = discount\n",
    "\n",
    "    def get_qvalue(self, state, action):\n",
    "        \"\"\" Returns Q(state,action) \"\"\"\n",
    "        return self._qvalues[state][action]\n",
    "\n",
    "    def set_qvalue(self, state, action, value):\n",
    "        \"\"\" Sets the Qvalue for [state, action] to the given value \"\"\"\n",
    "        self._qvalues[state][action] = value\n",
    "\n",
    "    def get_value(self, state):\n",
    "        \"\"\"\n",
    "        Compute your agent's estimate of V(s):\n",
    "            V(s) = max_over_action Q(state, action).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)  # please take into account that q-values can be negative\n",
    "\n",
    "        # If there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        value = max(self.get_qvalue(state, possible_action) for possible_action in possible_actions)\n",
    "        \n",
    "        return value\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "            Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "        \"\"\"\n",
    "\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        lr = self.alpha  # learning rate\n",
    "        \n",
    "        qvalue = (1 - lr) * self.get_qvalue(state, action) + lr * (reward + gamma * self.get_value(next_state))\n",
    "        \n",
    "        self.set_qvalue(state, action, qvalue)\n",
    "\n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the best action to take in a state (using current q-values).\n",
    "        \"\"\"\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "        \n",
    "        q_dict = {possible_action: self.get_qvalue(state, possible_action) for possible_action in possible_actions}\n",
    "    \n",
    "        max_q = max(q_dict.values())\n",
    "        best_actions = [action for action, q in q_dict.items() if q == max_q]\n",
    "        best_action = random.choice(best_actions)\n",
    "\n",
    "        return best_action\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Compute the action to take in the current state, including exploration.  \n",
    "        With probability self.epsilon, we should take a random action.\n",
    "            otherwise - the best policy action (self.get_best_action).\n",
    "        \"\"\"\n",
    "\n",
    "        # Pick Action\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "        action = None\n",
    "\n",
    "        # If there are no legal actions, return None\n",
    "        if len(possible_actions) == 0:\n",
    "            return None\n",
    "\n",
    "        # agent parameters\n",
    "        epsilon = self.epsilon\n",
    "        \n",
    "        if random.random() < self.epsilon:\n",
    "            chosen_action = random.choice(possible_actions)\n",
    "        else:\n",
    "            chosen_action = self.get_best_action(state)\n",
    "\n",
    "        return chosen_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "The state-action value update is given by:\n",
    "\n",
    "$$\n",
    "Q({S_t},{A_t}) \\leftarrow Q({S_t},{A_t}) + \\alpha \\left[{R_{t+1}} + \\gamma Q({S_{t+1}},{A_{t+1}}) - Q({S_t},{A_t})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent(QLearningAgent):\n",
    "    \"\"\" SARSA Agent \"\"\"\n",
    "    \n",
    "    def sarsa_update(self, state, action, reward, next_state,next_action):\n",
    "        \"\"\"\n",
    "        You should do your Q-Value update here:\n",
    "           Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))\n",
    "           Q(s,a) := Q(s,a) + alpha(R+ gamma * Q (s',a') - Q(s,a))\n",
    "        \"\"\"\n",
    "        # agent parameters\n",
    "        gamma = self.discount\n",
    "        learning_rate = self.alpha\n",
    "        \n",
    "        qvalue = (1 - learning_rate) * self.get_qvalue(state, action) + learning_rate *\\\n",
    "        (reward + gamma * self.get_qvalue(next_state, next_action))\n",
    "        \n",
    "        self.set_qvalue(state, action, qvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value SARSA\n",
    "\n",
    "**Expected Value SARSA** is an online algorithm - instead of taking the maximum action value for a state (in Q learning) it takes  expectations of the all the actions in a state.\n",
    "\n",
    "The state-action value update is given by:\n",
    "$$\n",
    "Q({S_t},{A_t}) \\leftarrow Q({S_t},{A_t}) + \\alpha \\left[{R_{t+1}} + \\gamma \\sum_{a}\\pi(a|S_{t+1})Q({S_{t+1}},a) - Q({S_t},{A_t})\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVSarsaAgent(QLearningAgent):\n",
    "    \"\"\" Expected Value SARSA Agent \"\"\"\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\" \n",
    "        Returns Vpi for current state under epsilon-greedy policy:\n",
    "          V_{pi}(s) = sum _{over a_i} {pi(a_i | s) * Q(s, a_i)}\n",
    "        \"\"\"\n",
    "        epsilon = self.epsilon\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # if there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        len_actions = len(possible_actions)\n",
    "        \n",
    "        state_value = 0\n",
    "        for possible_action in possible_actions:\n",
    "            if possible_action == self.get_best_action(state):\n",
    "                state_value += (epsilon / len_actions + (1 - epsilon)) * self.get_qvalue(state, possible_action)\n",
    "            else:\n",
    "                state_value += (epsilon / len_actions * self.get_qvalue(state, possible_action))\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Value Softmax SARSA\n",
    "\n",
    "$$ \\pi(a_i|s) = softmax({Q(s,a_i) \\over \\tau}) = {e ^ {Q(s,a_i)/ \\tau}  \\over {\\sum_{a_j}  e ^{Q(s,a_j) / \\tau }}} $$\n",
    "\n",
    "* Sample <s,a,r,s'> from environment\n",
    "* Compute $\\hat{Q}(s,a) = r(s,a) + \\gamma\\mathbb{E}_{a_i \\sim\\pi(a|s')}Q(s',a_i)$ with a probability of {a_i \\sim\\pi(a|s')} being **softmax policy**\n",
    "* Update $Q(s,a) \\leftarrow (1-\\alpha)*Q(s,a) + \\alpha*\\hat{Q}(s,a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EVSoftmaxSarsaAgent(QLearningAgent):\n",
    "    \"\"\" Expected Value Softmax SARSA Agent \"\"\"\n",
    "\n",
    "    def get_value(self, state):\n",
    "        epsilon = self.epsilon\n",
    "        possible_actions = self.get_legal_actions(state)\n",
    "\n",
    "        # if there are no legal actions, return 0.0\n",
    "        if len(possible_actions) == 0:\n",
    "            return 0.0\n",
    "        \n",
    "        qvalues = np.array([self.get_qvalue(state, possible_action) for possible_action in possible_actions])\n",
    "\n",
    "        exp_sum = sum(np.exp(qvalues))\n",
    "        probabilities = [np.exp(q) / exp_sum for q in qvalues]\n",
    "        \n",
    "        state_value = sum(probabilities[i] * qvalues[i] for i in range(len(possible_actions)))\n",
    "        \n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Play and Train Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train(env, agent, t_max=10**4):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "\n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state s.\n",
    "        action = agent.get_action(state)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # train (update) agent for state s\n",
    "        agent.update(state, action, reward, next_state)\n",
    "        \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_and_train_sarsa(env, agent, t_max=10**4):\n",
    "    total_reward = 0.0\n",
    "    state = env.reset()\n",
    "    action = agent.get_action(state)  # this is the difference from Q learning\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        # get agent to pick action given state\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_action = agent.get_action(next_state)  # this is another difference from Q learning\n",
    "        \n",
    "        agent.sarsa_update(state, action, reward, next_state, next_action)\n",
    "        \n",
    "        state, action = next_state, next_action\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Temporal-Difference Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.envs.toy_text\n",
    "\n",
    "env = gym.envs.toy_text.CliffWalkingEnv()   # print(env.__doc__)\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "considered_epsilon = 0.2\n",
    "\n",
    "agent_ql = QLearningAgent(alpha=0.25, epsilon=considered_epsilon, discount=0.99,\n",
    "                          get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_sarsa = SarsaAgent(alpha=0.25, epsilon=considered_epsilon, discount=0.99,\n",
    "                           get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_ev_sarsa = EVSarsaAgent(alpha=0.25, epsilon=considered_epsilon, discount=0.99,\n",
    "                           get_legal_actions=lambda s: range(n_actions))\n",
    "\n",
    "agent_softmax = EVSoftmaxSarsaAgent(alpha=0.25, epsilon=considered_epsilon, discount=0.99,\n",
    "                          get_legal_actions=lambda s: range(n_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, span=100):\n",
    "    return pd.DataFrame({'x': np.asarray(x)}).ewm(span=span).mean().values\n",
    "\n",
    "rewards_ql, rewards_sarsa, rewards_ev_sarsa, rewards_softmax = [], [], [], []\n",
    "\n",
    "for i in range(10_000):\n",
    "    rewards_ql.append(play_and_train(env, agent_ql))\n",
    "    rewards_sarsa.append(play_and_train_sarsa(env, agent_sarsa))\n",
    "    rewards_ev_sarsa.append(play_and_train(env, agent_ev_sarsa))\n",
    "    rewards_softmax.append(play_and_train(env, agent_softmax))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        clear_output(True)\n",
    "        print('QLEARNING mean reward =', np.mean(rewards_ql[-100:]))\n",
    "        print('SARSA mean reward =', np.mean(rewards_sarsa[-100:]))\n",
    "        print('EVSARSA mean reward =', np.mean(rewards_ev_sarsa[-100:]))\n",
    "        print('SOFTMAX mean reward =', np.mean(rewards_softmax[-100:]))\n",
    "        \n",
    "        plt.title(\"epsilon = %s\" % agent_ql.epsilon)\n",
    "        plt.plot(moving_average(rewards_ql), label='Q-learning')\n",
    "        plt.plot(moving_average(rewards_sarsa), label='SARSA')\n",
    "        plt.plot(moving_average(rewards_ev_sarsa), label='Expected Value SARSA')\n",
    "        plt.plot(moving_average(rewards_softmax), label='Expected Value Softmax SARSA')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.ylim(-500, 0)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "1. Practical RL: Higher School of Economics (HSE), Yandex. Link: https://github.com/yandexdataschool/Practical_RL\n",
    "2. Richard S. Sutton and Andrew G. Barto, \"Reinforcement Learning: An Introduction\", The MIT Press, Cambridge, Massachusetts, London, England"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
